{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike Sharing Demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Features from the Bike Sharing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-1688a248413a>:6 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-1688a248413a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\opt\\spark-2.4.0-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \"\"\"\n\u001b[0;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mC:\\opt\\spark-2.4.0-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    312\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 314\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    315\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-1688a248413a>:6 "
     ]
    }
   ],
   "source": [
    "#Initializing PySpark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first remove the headers by using the 'sed' command:\n",
    "# sed 1d hour.csv > hour_noheader.csv\n",
    "path = \"hour_noheader.csv\"\n",
    "raw_data = sc.textFile(path)\n",
    "num_data = raw_data.count()\n",
    "records = raw_data.map(lambda x: x.split(\",\"))\n",
    "first = records.first()\n",
    "print (first)\n",
    "print (num_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the variables we want to keep\n",
    "\n",
    "_All variables:_\n",
    "\n",
    "`instant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt`\n",
    "\n",
    "_Variables to keep:_\n",
    "\n",
    "`season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,cnt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feature mappings for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cache the dataset to speed up subsequent operations\n",
    "records.cache()\n",
    "# function to get the categorical feature mapping for a given variable column\n",
    "def get_mapping(rdd, idx):\n",
    "    return rdd.map(lambda fields: fields[idx]).distinct().zipWithIndex().collectAsMap()\n",
    "\n",
    "# we want to extract the feature mappings for columns 2 - 9\n",
    "# try it out on column 2 first\n",
    "print (\"Mapping of first categorical feasture column: %s\" % get_mapping(records, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract all the catgorical mappings\n",
    "mappings = [get_mapping(records, i) for i in range(2,10)]\n",
    "cat_len = sum(map(len, mappings))\n",
    "num_len = len(records.first()[11:15])\n",
    "total_len = num_len + cat_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# required imports\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np\n",
    "\n",
    "# function to use the feature mappings to extract binary feature vectors, and concatenate with\n",
    "# the numerical feature vectors\n",
    "def extract_features(record):\n",
    "    cat_vec = np.zeros(cat_len)\n",
    "    i = 0\n",
    "    step = 0\n",
    "    for field in record[2:9]:\n",
    "        m = mappings[i]\n",
    "        idx = m[field]\n",
    "        cat_vec[idx + step] = 1\n",
    "        i = i + 1\n",
    "        step = step + len(m)\n",
    "    \n",
    "    num_vec = np.array([float(field) for field in record[10:14]])\n",
    "    return np.concatenate((cat_vec, num_vec))\n",
    "\n",
    "# function to extract the label from the last column\n",
    "def extract_label(record):\n",
    "    return float(record[-1])\n",
    "\n",
    "data = records.map(lambda r: LabeledPoint(extract_label(r), extract_features(r)))\n",
    "first_point = data.first()\n",
    "print \"Raw data: \" + str(first[2:])\n",
    "print \"Label: \" + str(first_point.label)\n",
    "print \"Linear Model feature vector:\\n\" + str(first_point.features)\n",
    "print \"Linear Model feature vector length: \" + str(len(first_point.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we need a separate set of feature vectors for the decision tree\n",
    "def extract_features_dt(record):\n",
    "    return np.array(map(float, record[2:14]))\n",
    "    \n",
    "data_dt = records.map(lambda r: LabeledPoint(extract_label(r), extract_features_dt(r)))\n",
    "first_point_dt = data_dt.first()\n",
    "print \"Decision Tree feature vector: \" + str(first_point_dt.features)\n",
    "print \"Decision Tree feature vector length: \" + str(len(first_point_dt.features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "help(LinearRegressionWithSGD.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(DecisionTree.trainRegressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Regression Model on the Bike Sharing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linear_model = LinearRegressionWithSGD.train(data, iterations=10, step=0.1, intercept=False)\n",
    "true_vs_predicted = data.map(lambda p: (p.label, linear_model.predict(p.features)))\n",
    "print \"Linear Model predictions: \" + str(true_vs_predicted.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we pass in an mepty mapping for categorical feature size {}\n",
    "dt_model = DecisionTree.trainRegressor(data_dt, {})\n",
    "preds = dt_model.predict(data_dt.map(lambda p: p.features))\n",
    "actual = data.map(lambda p: p.label)\n",
    "true_vs_predicted_dt = actual.zip(preds)\n",
    "print \"Decision Tree predictions: \" + str(true_vs_predicted_dt.take(5))\n",
    "print \"Decision Tree depth: \" + str(dt_model.depth())\n",
    "print \"Decision Tree number of nodes: \" + str(dt_model.numNodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perfomance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up performance metrics functions \n",
    "\n",
    "def squared_error(actual, pred):\n",
    "    return (pred - actual)**2\n",
    "\n",
    "def abs_error(actual, pred):\n",
    "    return np.abs(pred - actual)\n",
    "\n",
    "def squared_log_error(pred, actual):\n",
    "    return (np.log(pred + 1) - np.log(actual + 1))**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute performance metrics for linear model\n",
    "mse = true_vs_predicted.map(lambda (t, p): squared_error(t, p)).mean()\n",
    "mae = true_vs_predicted.map(lambda (t, p): abs_error(t, p)).mean()\n",
    "rmsle = np.sqrt(true_vs_predicted.map(lambda (t, p): squared_log_error(t, p)).mean())\n",
    "print \"Linear Model - Mean Squared Error: %2.4f\" % mse\n",
    "print \"Linear Model - Mean Absolute Error: %2.4f\" % mae\n",
    "print \"Linear Model - Root Mean Squared Log Error: %2.4f\" % rmsle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute performance metrics for decision tree model\n",
    "mse_dt = true_vs_predicted_dt.map(lambda (t, p): squared_error(t, p)).mean()\n",
    "mae_dt = true_vs_predicted_dt.map(lambda (t, p): abs_error(t, p)).mean()\n",
    "rmsle_dt = np.sqrt(true_vs_predicted_dt.map(lambda (t, p): squared_log_error(t, p)).mean())\n",
    "print \"Decision Tree - Mean Squared Error: %2.4f\" % mse_dt\n",
    "print \"Decision Tree - Mean Absolute Error: %2.4f\" % mae_dt\n",
    "print \"Decision Tree - Root Mean Squared Log Error: %2.4f\" % rmsle_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the categorical feature mapping for Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we create the categorical feature mapping for decision trees\n",
    "cat_features = dict([(i - 2, len(get_mapping(records, i)) + 1) for i in range(2,10)])\n",
    "print \"Categorical feature size mapping %s\" % cat_features\n",
    "# train the model again\n",
    "dt_model_2 = DecisionTree.trainRegressor(data_dt, categoricalFeaturesInfo=cat_features)\n",
    "preds_2 = dt_model_2.predict(data_dt.map(lambda p: p.features))\n",
    "actual_2 = data.map(lambda p: p.label)\n",
    "true_vs_predicted_dt_2 = actual_2.zip(preds_2)\n",
    "# compute performance metrics for decision tree model\n",
    "mse_dt_2 = true_vs_predicted_dt_2.map(lambda (t, p): squared_error(t, p)).mean()\n",
    "mae_dt_2 = true_vs_predicted_dt_2.map(lambda (t, p): abs_error(t, p)).mean()\n",
    "rmsle_dt_2 = np.sqrt(true_vs_predicted_dt_2.map(lambda (t, p): squared_log_error(t, p)).mean())\n",
    "print \"Decision Tree - Mean Squared Error: %2.4f\" % mse_dt_2\n",
    "print \"Decision Tree - Mean Absolute Error: %2.4f\" % mae_dt_2\n",
    "print \"Decision Tree - Root Mean Squared Log Error: %2.4f\" % rmsle_dt_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming the Target Variable\n",
    "## Distributon of Raw Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "targets = records.map(lambda r: float(r[-1])).collect()\n",
    "hist(targets, bins=40, color='lightblue', normed=True)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(16, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Log Transformed Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_targets = records.map(lambda r: np.log(float(r[-1]))).collect()\n",
    "hist(log_targets, bins=40, color='lightblue', normed=True)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(16, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Square-root Transformed target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqrt_targets = records.map(lambda r: np.sqrt(float(r[-1]))).collect()\n",
    "hist(sqrt_targets, bins=40, color='lightblue', normed=True)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(16, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of Training on Log Transformed Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train a linear model on log-transformed targets\n",
    "data_log = data.map(lambda lp: LabeledPoint(np.log(lp.label), lp.features))\n",
    "model_log = LinearRegressionWithSGD.train(data_log, iterations=10, step=0.1)\n",
    "\n",
    "true_vs_predicted_log = data_log.map(lambda p: (np.exp(p.label), np.exp(model_log.predict(p.features))))\n",
    "\n",
    "mse_log = true_vs_predicted_log.map(lambda (t, p): squared_error(t, p)).mean()\n",
    "mae_log = true_vs_predicted_log.map(lambda (t, p): abs_error(t, p)).mean()\n",
    "rmsle_log = np.sqrt(true_vs_predicted_log.map(lambda (t, p): squared_log_error(t, p)).mean())\n",
    "print \"Mean Squared Error: %2.4f\" % mse_log\n",
    "print \"Mean Absolute Error: %2.4f\" % mae_log\n",
    "print \"Root Mean Squared Log Error: %2.4f\" % rmsle_log\n",
    "print \"Non log-transformed predictions:\\n\" + str(true_vs_predicted.take(3))\n",
    "print \"Log-transformed predictions:\\n\" + str(true_vs_predicted_log.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train a decision tree model on log-transformed targets\n",
    "data_dt_log = data_dt.map(lambda lp: LabeledPoint(np.log(lp.label), lp.features))\n",
    "dt_model_log = DecisionTree.trainRegressor(data_dt_log, {})\n",
    "\n",
    "preds_log = dt_model_log.predict(data_dt_log.map(lambda p: p.features))\n",
    "actual_log = data_dt_log.map(lambda p: p.label)\n",
    "true_vs_predicted_dt_log = actual_log.zip(preds_log).map(lambda (t, p): (np.exp(t), np.exp(p)))\n",
    "\n",
    "mse_log_dt = true_vs_predicted_dt_log.map(lambda (t, p): squared_error(t, p)).mean()\n",
    "mae_log_dt = true_vs_predicted_dt_log.map(lambda (t, p): abs_error(t, p)).mean()\n",
    "rmsle_log_dt = np.sqrt(true_vs_predicted_dt_log.map(lambda (t, p): squared_log_error(t, p)).mean())\n",
    "print \"Mean Squared Error: %2.4f\" % mse_log_dt\n",
    "print \"Mean Absolute Error: %2.4f\" % mae_log_dt\n",
    "print \"Root Mean Squared Log Error: %2.4f\" % rmsle_log_dt\n",
    "print \"Non log-transformed predictions:\\n\" + str(true_vs_predicted_dt.take(3))\n",
    "print \"Log-transformed predictions:\\n\" + str(true_vs_predicted_dt_log.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation\n",
    "## Creating Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create training and testing sets for linear model\n",
    "data_with_idx = data.zipWithIndex().map(lambda (k, v): (v, k))\n",
    "test = data_with_idx.sample(False, 0.2, 42)\n",
    "train = data_with_idx.subtractByKey(test)\n",
    "\n",
    "train_data = train.map(lambda (idx, p): p)\n",
    "test_data = test.map(lambda (idx, p) : p)\n",
    "\n",
    "train_size = train_data.count()\n",
    "test_size = test_data.count()\n",
    "print \"Training data size: %d\" % train_size\n",
    "print \"Test data size: %d\" % test_size\n",
    "print \"Total data size: %d \" % num_data\n",
    "print \"Train + Test size : %d\" % (train_size + test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create training and testing sets for decision tree\n",
    "data_with_idx_dt = data_dt.zipWithIndex().map(lambda (k, v): (v, k))\n",
    "test_dt = data_with_idx_dt.sample(False, 0.2, 42)\n",
    "train_dt = data_with_idx_dt.subtractByKey(test_dt)\n",
    "\n",
    "train_data_dt = train_dt.map(lambda (idx, p): p)\n",
    "test_data_dt = test_dt.map(lambda (idx, p) : p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Impact of Different Parameter Settings for Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a function to evaluate linear model\n",
    "def evaluate(train, test, iterations, step, regParam, regType, intercept):\n",
    "    model = LinearRegressionWithSGD.train(train, iterations, step, regParam=regParam, regType=regType, intercept=intercept)\n",
    "    tp = test.map(lambda p: (p.label, model.predict(p.features)))\n",
    "    rmsle = np.sqrt(tp.map(lambda (t, p): squared_log_error(t, p)).mean())\n",
    "    return rmsle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = [1, 5, 10, 20, 50, 100]\n",
    "metrics = [evaluate(train_data, test_data, param, 0.01, 0.0, 'l2', False) for param in params]\n",
    "print params\n",
    "print metrics\n",
    "plot(params, metrics)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "pyplot.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = [0.01, 0.025, 0.05, 0.1, 1.0]\n",
    "metrics = [evaluate(train_data, test_data, 10, param, 0.0, 'l2', False) for param in params]\n",
    "print params\n",
    "print metrics\n",
    "plot(params, metrics)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "pyplot.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = [0.0, 0.01, 0.1, 1.0, 5.0, 10.0, 20.0]\n",
    "metrics = [evaluate(train_data, test_data, 10, 0.1, param, 'l2', False) for param in params]\n",
    "print params\n",
    "print metrics\n",
    "plot(params, metrics)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "pyplot.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = [0.0, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "metrics = [evaluate(train_data, test_data, 10, 0.1, param, 'l1', False) for param in params]\n",
    "print params\n",
    "print metrics\n",
    "plot(params, metrics)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "pyplot.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Investigate sparsity of L1-regularized solution\n",
    "model_l1 = LinearRegressionWithSGD.train(train_data, 10, 0.1, regParam=1.0, regType='l1', intercept=False)\n",
    "model_l1_10 = LinearRegressionWithSGD.train(train_data, 10, 0.1, regParam=10.0, regType='l1', intercept=False)\n",
    "model_l1_100 = LinearRegressionWithSGD.train(train_data, 10, 0.1, regParam=100.0, regType='l1', intercept=False)\n",
    "print \"L1 (1.0) number of zero weights: \" + str(sum(model_l1.weights.array == 0))\n",
    "print \"L1 (10.0) number of zeros weights: \" + str(sum(model_l1_10.weights.array == 0))\n",
    "print \"L1 (100.0) number of zeros weights: \" + str(sum(model_l1_100.weights.array == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = [False, True]\n",
    "metrics = [evaluate(train_data, test_data, 10, 0.1, 1.0, 'l2', param) for param in params]\n",
    "print params\n",
    "print metrics\n",
    "bar(params, metrics, color='lightblue')\n",
    "fig = matplotlib.pyplot.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Impact of Different Parameter Settings for Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a function to evaluate decision tree model\n",
    "def evaluate_dt(train, test, maxDepth, maxBins):\n",
    "    model = DecisionTree.trainRegressor(train, {}, impurity='variance', maxDepth=maxDepth, maxBins=maxBins)\n",
    "    preds = model.predict(test.map(lambda p: p.features))\n",
    "    actual = test.map(lambda p: p.label)\n",
    "    tp = actual.zip(preds)\n",
    "    rmsle = np.sqrt(tp.map(lambda (t, p): squared_log_error(t, p)).mean())\n",
    "    return rmsle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = [1, 2, 3, 4, 5, 10, 20]\n",
    "metrics = [evaluate_dt(train_data_dt, test_data_dt, param, 32) for param in params]\n",
    "print params\n",
    "print metrics\n",
    "plot(params, metrics)\n",
    "fig = matplotlib.pyplot.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = [2, 4, 8, 16, 32, 64, 100]\n",
    "metrics = [evaluate_dt(train_data_dt, test_data_dt, 5, param) for param in params]\n",
    "print params\n",
    "print metrics\n",
    "plot(params, metrics)\n",
    "fig = matplotlib.pyplot.gcf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
